{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "synthetic_FT_TRansformer.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crOIIvKNlt4N"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pytorch-widedeep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# widedeep\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import sklearn.datasets as dt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from pytorch_widedeep.models import FTTransformer, SAINT, TabFastFormer, TabPerceiver, TabTransformer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def set_total_seed(seed = 42):\n",
        "  # Set random seed\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed);\n",
        "\n",
        "#functions\n",
        "\n",
        "def epistemic_uncertainty(model, X_test, x_train, y_train, T):\n",
        "\n",
        "    probs_mc_dropout = []\n",
        "    for _ in tqdm(range(T)):\n",
        "        out = model(X_test).detach()\n",
        "        probs_mc_dropout += [out.view(out.shape[0])]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    predictive_mean = np.mean(props, axis=0)\n",
        "    predictive_variance = np.var(props, axis=0)\n",
        "\n",
        "    #plt.plot(x_test, y_test, ls='--')\n",
        "    plt.scatter(x_train, y_train, color='black')\n",
        "    plt.errorbar(x_test, predictive_mean, yerr=predictive_variance, fmt='.', color = \"blue\")\n",
        "\n",
        "def epistemic_uncertainty_wo_image(model, X_test, x_train, y_train, T):\n",
        "\n",
        "    probs_mc_dropout = []\n",
        "    for _ in tqdm(range(T)):\n",
        "        out = model(X_test).detach()\n",
        "        probs_mc_dropout += [out.view(out.shape[0])]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    predictive_mean = np.mean(props, axis=0)\n",
        "    predictive_variance = np.var(props, axis=0)\n",
        "\n",
        "    return predictive_mean, predictive_variance\n",
        "\n",
        "def aleatoric_loss(y_true, y_pred):\n",
        "    # 2 columns predicted: 1 is value, 2 is it's variance (std)\n",
        "    N = y_true.shape[0]\n",
        "    se = torch.pow((y_true[:,0]-y_pred[:,0]),2)\n",
        "    inv_std = torch.exp((-1) * y_pred[:,1])\n",
        "    loss1 = torch.mean(inv_std*se)\n",
        "    loss2 = torch.mean(y_pred[:,1], dim = 0)\n",
        "    return 0.5*(loss1 + loss2)\n",
        "\n",
        "def pred_mean_and_epistemic_std(model, X_test, x_train, y_train, T):\n",
        "\n",
        "    probs_mc_dropout = []\n",
        "    for _ in tqdm(range(T)):\n",
        "        out = model(X_test).detach()[:,0]\n",
        "        \n",
        "        probs_mc_dropout += [out.view(out.shape[0])]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    predictive_mean = np.mean(props, axis=0)\n",
        "    predictive_variance = np.var(props, axis=0)\n",
        "\n",
        "    return predictive_mean, predictive_variance\n",
        "\n",
        "\n",
        "def aleatoric_std(model, X_test, T=40):\n",
        "    probs_mc_dropout = []\n",
        "    for _ in range(T):\n",
        "        probs_mc_dropout += [model(X_test).detach()[:,1]]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    aleatoric_std = np.exp(0.5*np.mean(props, axis=0))\n",
        "    \n",
        "    \n",
        "    return aleatoric_std"
      ],
      "metadata": {
        "id": "PpbAAPX0mIhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "n = 6000\n",
        "test_n = 1000\n",
        "seed = 11\n",
        "rand_state = 11\n",
        "test_rand_state = 13\n",
        "\n",
        "\n",
        "### data\n",
        "x,y = dt.make_regression(n_samples=n,\n",
        "                         n_features=4,\n",
        "                         noise=0,\n",
        "                         random_state=rand_state)\n",
        "\n",
        "\n",
        "x_1,y_1 = dt.make_friedman1(n_samples=n,n_features=5,random_state=rand_state)\n",
        "x_2,y_2 = dt.make_friedman2(n_samples=n,random_state=rand_state)\n",
        "x_3,y_3 = dt.make_friedman3(n_samples=n,random_state=rand_state)\n",
        "\n",
        "\n",
        "x_t,y_t = dt.make_regression(n_samples=test_n,\n",
        "                         n_features=4,\n",
        "                         noise=0,\n",
        "                         random_state=test_rand_state)\n",
        "\n",
        "\n",
        "x_t1,y_t1 = dt.make_friedman1(n_samples=test_n, n_features=5, random_state=test_rand_state)\n",
        "x_t2,y_t2 = dt.make_friedman2(n_samples=test_n, random_state=test_rand_state)\n",
        "x_t3,y_t3 = dt.make_friedman3(n_samples=test_n, random_state=test_rand_state)\n",
        "\n",
        "### data\n",
        "x_train_poly = np.random.uniform(-7, -2, 2000)\n",
        "y_train_poly = np.random.normal(scale=.05, size=x_train_poly.shape) + 3 \n",
        "\n",
        "x_train_poly = np.concatenate([x_train_poly, np.random.uniform(5, 10, 2000)])\n",
        "y_train_poly = np.concatenate([y_train_poly, np.random.normal(scale=.1, size=x_train_poly[2000:].shape)])+ 3\n",
        "\n",
        "x_train_poly = np.concatenate([x_train_poly, np.random.uniform(15, 20, 2000)])\n",
        "y_train_poly = np.concatenate([y_train_poly, np.random.normal(scale=.1, size=x_train_poly[4000:].shape)])+3\n",
        "\n",
        "train_data = (torch.stack((torch.tensor(x_train_poly), torch.tensor(np.array(list([2]*2000+[0]*2000+[1]*2000)))), dim = 0).T).numpy()\n",
        "train_data = torch.hstack([torch.tensor(train_data), torch.tensor(np.array(list([0]*6000))).unsqueeze(0).T]).detach().numpy()\n",
        "# Standart scaling\n",
        "#train_data = torch.hstack([torch.tensor(np.hstack([x, x_1, x_2, x_3])), torch.tensor(train_data)])\n",
        "train_data = torch.hstack([torch.tensor(StandardScaler().fit_transform(np.hstack([x, x_1, x_2, x_3]))), torch.tensor(train_data)])\n",
        "\n",
        "\n",
        "x_train = train_data\n",
        "y_train = y_train_poly\n",
        "\n",
        "x_test = np.linspace(-10,25,1000)\n",
        "x_test = (torch.stack((torch.tensor(x_test), torch.tensor(np.array(list([2]*333+[0]*333+[1]*334)))), dim = 0).T).numpy()\n",
        "x_test = torch.reshape(torch.tensor(x_test), (-1, 2))\n",
        "x_test = torch.hstack([x_test, torch.tensor(np.array(list([0]*1000))).unsqueeze(0).T])\n",
        "\n",
        "# Standart scaling\n",
        "#x_test = torch.hstack([torch.tensor(np.hstack([x_t, x_t1, x_t2, x_t3])), torch.tensor(x_test)])\n",
        "x_test = torch.hstack([torch.tensor(StandardScaler().fit_transform(np.hstack([x_t, x_t1, x_t2, x_t3]))), torch.tensor(x_test)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(x_train_poly, y_train_poly);\n",
        "#parameters\n",
        "verbose = False\n",
        "EPOCHS, BATCH_SIZE = 40, 16\n",
        "\n",
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "colnames = [\"num_{}\".format(i) for i in range(1, 19)]\n",
        "colnames.append('cat_0')\n",
        "colnames.append('cat_1')\n",
        " \n",
        "\n",
        "continuous_cols = [\"num_{}\".format(i) for i in range(1, 19)]\n",
        "categorical_cols = ['cat_0', 'cat_1']\n",
        "cat_embed_input = [(u,i) for u,i in zip(colnames[-2:], [1]*2)]\n",
        "column_idx = {k:v for v,k in enumerate(colnames)}\n",
        "\n",
        "\n",
        "# default dropout set\n",
        "model = FTTransformer(column_idx=column_idx,  \n",
        "                      cat_embed_input=cat_embed_input,\n",
        "                      continuous_cols=continuous_cols,\n",
        "                      n_blocks=2, n_heads = 8, input_dim = 32, \n",
        "                      cat_embed_dropout = 0.0, cont_embed_dropout = 0.0,\n",
        "                      attn_dropout = 0.0, ff_dropout = 0.1,\n",
        "                      mlp_activation = \"leaky_relu\",\n",
        "                      mlp_dropout = 0.0, mlp_hidden_dims = [64, 32, 16, 2])\n",
        "\n",
        "\n",
        "# TRAIN\n",
        "# optimizers\n",
        "optimizer = Adam(model.parameters(), lr = 1e-4)\n",
        "\n",
        "\n",
        "\n",
        "# data and dataloader\n",
        "dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
        "data_train = DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "# criterion and device\n",
        "#criterion = nn.MSELoss()\n",
        "device = 'cpu'\n",
        "loss_dict = {}\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in tqdm(range(0, EPOCHS)): # 100 epochs at maximum\n",
        "  \n",
        "  # Print epoch\n",
        "  if verbose:\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Set current loss value\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(data_train, 0):\n",
        "    \n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    targets = targets.float()\n",
        "    \n",
        "\n",
        "    if len(inputs.shape) == 1:\n",
        "      inputs = inputs.reshape(inputs.shape[0], 1)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    \n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = aleatoric_loss(targets, outputs)\n",
        "    \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    loss_dict[epoch] = current_loss\n",
        "    if verbose:\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 50))\n",
        "      current_loss = 0.0"
      ],
      "metadata": {
        "id": "TLQItaSomMsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(loss_dict.keys()), list(loss_dict.values()))\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\");"
      ],
      "metadata": {
        "id": "tAXKKyVnmNj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, epistemic_std = pred_mean_and_epistemic_std(model = model,\n",
        "                                                  X_test = x_test,\n",
        "                                                  x_train = x_train,\n",
        "                                                  y_train = y_train,\n",
        "                                                  T = 50)\n",
        "\n",
        "a_std = aleatoric_std(model, X_test=x_test, T=50)\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "#_=plt.scatter(x_test, y_test, ls='--')\n",
        "pca = PCA(n_components=1)\n",
        "train_pca_x = pca.fit_transform(x_train)\n",
        "test_pca_x = pca.fit_transform(x_test)\n",
        "\n",
        "_=plt.scatter(train_pca_x, y_train, color='black')\n",
        "_=plt.errorbar(test_pca_x[::-1], mean, yerr=a_std, fmt='.', color = \"green\", alpha =0.3)\n",
        "_=plt.title('Prediction with Aleatoric and Epistemic std')\n",
        "\n",
        "_=plt.errorbar(test_pca_x[::-1], mean, yerr=epistemic_std, fmt='.', color = \"blue\", alpha =0.3)"
      ],
      "metadata": {
        "id": "y4osftNkmNmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,7))\n",
        "plt.plot(epistemic_std, label = \"epistimic\")\n",
        "plt.plot(a_std, label = \"aleatoric\")\n",
        "plt.legend();\n"
      ],
      "metadata": {
        "id": "_mJeUEsMmNos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bbQGAkzomkbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}