{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FT-Transformer_experiment.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GeUntmXdv_-"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pytorch-widedeep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# widedeep\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from pytorch_widedeep.models import FTTransformer, SAINT, TabFastFormer, TabPerceiver, TabTransformer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def set_total_seed(seed = 42):\n",
        "  # Set random seed\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed);\n",
        "\n",
        "#functions\n",
        "\n",
        "def epistemic_uncertainty(model, X_test, x_train, y_train, T):\n",
        "\n",
        "    probs_mc_dropout = []\n",
        "    for _ in tqdm(range(T)):\n",
        "        out = model(X_test).detach()\n",
        "        probs_mc_dropout += [out.view(out.shape[0])]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    predictive_mean = np.mean(props, axis=0)\n",
        "    predictive_variance = np.var(props, axis=0)\n",
        "\n",
        "    #plt.plot(x_test, y_test, ls='--')\n",
        "    plt.scatter(x_train, y_train, color='black')\n",
        "    plt.errorbar(x_test, predictive_mean, yerr=predictive_variance, fmt='.', color = \"blue\")\n",
        "\n",
        "def epistemic_uncertainty_wo_image(model, X_test, x_train, y_train, T):\n",
        "\n",
        "    probs_mc_dropout = []\n",
        "    for _ in tqdm(range(T)):\n",
        "        out = model(X_test).detach()\n",
        "        probs_mc_dropout += [out.view(out.shape[0])]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    predictive_mean = np.mean(props, axis=0)\n",
        "    predictive_variance = np.var(props, axis=0)\n",
        "\n",
        "    return predictive_mean, predictive_variance\n",
        "\n",
        "def aleatoric_loss(y_true, y_pred):\n",
        "    # 2 columns predicted: 1 is value, 2 is it's variance (std)\n",
        "    N = y_true.shape[0]\n",
        "    se = torch.pow((y_true[:,0]-y_pred[:,0]),2)\n",
        "    inv_std = torch.exp((-1) * y_pred[:,1])\n",
        "    loss1 = torch.mean(inv_std*se)\n",
        "    loss2 = torch.mean(y_pred[:,1], dim = 0)\n",
        "    return 0.5*(loss1 + loss2)\n",
        "\n",
        "def pred_mean_and_epistemic_std(model, X_test, x_train, y_train, T):\n",
        "\n",
        "    probs_mc_dropout = []\n",
        "    for _ in tqdm(range(T)):\n",
        "        out = model(X_test).detach()[:,0]\n",
        "        \n",
        "        probs_mc_dropout += [out.view(out.shape[0])]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    predictive_mean = np.mean(props, axis=0)\n",
        "    predictive_variance = np.var(props, axis=0)\n",
        "\n",
        "    return predictive_mean, predictive_variance\n",
        "\n",
        "\n",
        "def aleatoric_std(model, X_test=x_test, T=40):\n",
        "    probs_mc_dropout = []\n",
        "    for _ in range(T):\n",
        "        #out = model(X_test)\n",
        "\n",
        "        probs_mc_dropout += [model(X_test).detach()[:,1]]\n",
        "    props = np.array([t.detach().numpy() for t in probs_mc_dropout])\n",
        "\n",
        "    aleatoric_std = np.exp(0.5*np.mean(props, axis=0))\n",
        "    \n",
        "    \n",
        "    return aleatoric_std\n"
      ],
      "metadata": {
        "id": "i0jl5wxLdyAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### data\n",
        "\n",
        "x_train_poly = np.random.uniform(-7, -2, 2000)\n",
        "y_train_poly = np.random.normal(scale=.05, size=x_train_poly.shape) + 10\n",
        "\n",
        "x_train_poly = np.concatenate([x_train_poly, np.random.uniform(5, 10, 2000)])\n",
        "y_train_poly = np.concatenate([y_train_poly, np.random.normal(scale=10, size=x_train_poly[2000:].shape)]) + 20\n",
        "\n",
        "x_train_poly = np.concatenate([x_train_poly, np.random.uniform(15, 20, 2000)])\n",
        "y_train_poly = np.concatenate([y_train_poly, np.random.normal(scale=5, size=x_train_poly[4000:].shape)])+10\n",
        "\n",
        "#train_data = (torch.stack((torch.tensor(x_train_poly), torch.tensor(np.array(list([0]*2000+[1]*2000+[2]*2000)))), dim = 0).T).numpy()\n",
        "\n",
        "x_train = x_train_poly\n",
        "y_train = y_train_poly\n",
        "\n",
        "x_test = np.linspace(-10,25,2000)\n",
        "x_test = torch.reshape(torch.tensor(x_test), (-1, 1))\n",
        "\n",
        "plt.scatter(x_train_poly, y_train_poly);\n",
        "#parameters\n",
        "verbose = True\n",
        "EPOCHS, BATCH_SIZE = 80, 64\n",
        "\n",
        "# Set fixed random number seed\n",
        "set_total_seed(42)\n",
        "\n",
        "#Model FT-Transformer\n",
        "# parameter definition\n",
        "colnames = ['num']\n",
        "continuous_cols = ['num']\n",
        "column_idx = {k:v for v,k in enumerate(colnames)}\n",
        "\n",
        "\n",
        "# default dropout set\n",
        "model = FTTransformer(column_idx=column_idx,  continuous_cols=continuous_cols,\n",
        "                      n_blocks=2, n_heads = 8, input_dim = 32, \n",
        "                      cat_embed_dropout = 0.0, cont_embed_dropout = 0.0,\n",
        "                      attn_dropout = 0.0, ff_dropout = 0.1,\n",
        "                      mlp_activation = \"leaky_relu\",\n",
        "                      mlp_dropout = 0.0, mlp_hidden_dims = [128, 64, 32, 16, 2])\n",
        "\n",
        "\n",
        "\n",
        "# TRAIN\n",
        "# optimizers\n",
        "optimizer = Adam(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# data and dataloader\n",
        "dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
        "data_train = DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "# criterion and device\n",
        "#criterion = nn.MSELoss()\n",
        "device = 'cpu'\n",
        "loss_dict = {}\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in tqdm(range(0, EPOCHS)): # 100 epochs at maximum\n",
        "  \n",
        "  # Print epoch\n",
        "  if verbose:\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "  \n",
        "  # Set current loss value\n",
        "  current_loss = 0.0\n",
        "  \n",
        "  # Iterate over the DataLoader for training data\n",
        "  for i, data in enumerate(data_train, 0):\n",
        "    \n",
        "    # Get and prepare inputs\n",
        "    inputs, targets = data\n",
        "    inputs, targets = inputs.float(), targets.float()\n",
        "    \n",
        "    if len(inputs.shape) == 1:\n",
        "      inputs = torch.reshape(inputs, (-1, 1))\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    targets = targets.reshape((targets.shape[0], 1))\n",
        "    \n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Perform forward pass\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = aleatoric_loss(targets, outputs)\n",
        "    \n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    loss_dict[epoch] = current_loss\n",
        "    if verbose:\n",
        "      if i % 10 == 0:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 50))\n",
        "      current_loss = 0.0\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NIeRG6r8d1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(loss_dict.keys()), list(loss_dict.values()))\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\");"
      ],
      "metadata": {
        "id": "I-plL7yod1y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aleatoric and epistemic uncertainty\n",
        "mean, epistemic_std = pred_mean_and_epistemic_std(model = model,\n",
        "                                                  X_test = x_test,\n",
        "                                                  x_train = x_train,\n",
        "                                                  y_train = y_train,\n",
        "                                                  T = 30)\n",
        "\n",
        "a_std = aleatoric_std(model, X_test=x_test, T=30)\n",
        "\n",
        "plt.figure(figsize = (12,7))\n",
        "\n",
        "_=plt.scatter(x_train, y_train, color='black')\n",
        "_=plt.errorbar(x_test, mean, yerr=a_std, fmt='.', label = \"aleatoric\" ,color = \"green\", alpha =0.1)\n",
        "_=plt.title('Prediction with Aleatoric and Epistemic std')\n",
        "\n",
        "_=plt.errorbar(x_test, mean, yerr=epistemic_std, fmt='.', label = \"epistemic\", color = \"blue\", alpha =0.1)\n",
        "_=plt.legend()"
      ],
      "metadata": {
        "id": "ySTwL6ACd2i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,7))\n",
        "plt.plot(epistemic_std, label = \"epistimic\")\n",
        "plt.plot(a_std, label = \"aleatoric\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "W4T24WUnd2lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vdOw04GWd2qQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}