{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies and libraries"
      ],
      "metadata": {
        "id": "9NqfvaUqCmel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2RX38Hg7sYf"
      },
      "outputs": [],
      "source": [
        "! pip install pytorch_tabular\n",
        "! git clone https://github.com/manujosephv/pytorch_tabular\n",
        "%cd pytorch_tabular\n",
        "\n",
        "!python setup.py install\n",
        "!pip install setuptools==59.5.0\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qmuotr7_UO0"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if not IN_COLAB:\n",
        "    os.chdir(\"..\")\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pytorch_tabular import TabularModel\n",
        "from pytorch_tabular.models import CategoryEmbeddingModelConfig, FTTransformerConfig, FTTransformerModel, TabNetModelConfig, TabNetModel, AutoIntConfig, AutoIntConfig, TabTransformerConfig, TabTransformerModel\n",
        "from pytorch_tabular.models import AutoIntModel, AutoIntConfig, NodeConfig, NODEModel\n",
        "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig, ModelConfig\n",
        "from pytorch_tabular.models import BaseModel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from omegaconf import DictConfig\n",
        "from typing import Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# for plots\n",
        "font = {'family': 'serif',\n",
        "        'color':  'darkred',\n",
        "        'weight': 'normal',\n",
        "        'size': 16\n",
        "        }\n",
        "\n",
        "font_title = {'family': 'serif',\n",
        "        'color':  'darkred',\n",
        "        'weight': 'normal',\n",
        "        'size': 20\n",
        "        }\n",
        "\n",
        "# Define functions\n",
        "\n",
        "def set_total_seed(seed = 42):\n",
        "  # Set random seed\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed);\n",
        "\n",
        "def make_data(num_samples = 10000,\n",
        "              x_1 = 10, x_2 = 6, x_3 = -1.5, x_error_2 = 1,\n",
        "              sin_koef = 3):\n",
        "\n",
        "  samples = num_samples\n",
        "  x_data = np.random.sample(samples)[:, np.newaxis].astype(np.float32)*5\n",
        "  y_data = np.add(x_1 * x_data+x_2 * x_data**2+x_3 * x_data**3, np.multiply(x_error_2*(x_data)**2, np.sin(sin_koef*x_data)*np.random.standard_normal(x_data.shape)))\n",
        "\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "  df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "  df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "  df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})\n",
        "\n",
        "  return df_train, df_valid, df_test\n",
        "\n",
        "def make_additional_data(num_samples = 10000,\n",
        "              x_1 = -2, x_2 = 4, x_3 = -0.5, x_error_2 = 2,\n",
        "              sin_koef = 1, shift = 7):\n",
        "\n",
        "  samples = num_samples\n",
        "  x_data = np.random.sample(samples)[:, np.newaxis].astype(np.float32)*5\n",
        "  y_data = np.add(x_1 * x_data+x_2 * x_data**2+x_3 * x_data**3, np.multiply(x_error_2*(x_data)**2, np.sin(sin_koef*x_data)*np.random.standard_normal(x_data.shape)))\n",
        "\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "  df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "  df_train['col1'] = df_train['col1'] + shift\n",
        "  df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "  df_valid['col1'] = df_valid['col1'] + shift\n",
        "  df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})\n",
        "  df_test['col1']= df_test['col1'] + shift\n",
        "  \n",
        "  df_train['col1'] = df_train['col1'].iloc[::-1]\n",
        "  df_valid['col1'] = df_valid['col1'].iloc[::-1]\n",
        "  df_test['col1'] = df_test['col1'].iloc[::-1]\n",
        "\n",
        "  return df_train, df_valid, df_test\n",
        "\n",
        "\n",
        "def setting_model_and_getting_predictions(model_config,\n",
        "                                          optimizer_config,\n",
        "                                          N_in_ensemble = 5, epochs = 10, \n",
        "                                          batch_size = 32,\n",
        "                                          num_col_names = ['col1'],\n",
        "                                          cat_col_names = ['cat1'], \n",
        "                                          auto_lr_find_for_trainer_config = False,\n",
        "                                          early_stopping_patience_for_trainer_config = 5,\n",
        "                                          GPU = -1):\n",
        "  \n",
        "  # sample subsamples for ensemble\n",
        "  indexes = np.array([random.sample(range(0, int(len(df_train))), int(len(df_train)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "  indexes_valid = np.array([random.sample(range(0, int(len(df_valid))), int(len(df_valid)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "\n",
        "  steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "\n",
        "  # data settings\n",
        "  data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names)\n",
        "\n",
        "  # define trainer configuration\n",
        "  trainer_config = TrainerConfig(\n",
        "    auto_lr_find = auto_lr_find_for_trainer_config, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size = batch_size,\n",
        "    max_epochs = epochs,\n",
        "    early_stopping_patience = early_stopping_patience_for_trainer_config,\n",
        "    gpus=GPU,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "  )\n",
        "\n",
        "  # construct model\n",
        "  tabular_model = TabularModel(\n",
        "      data_config = data_config,\n",
        "      model_config = model_config,\n",
        "      optimizer_config = optimizer_config,\n",
        "      trainer_config = trainer_config\n",
        "  )\n",
        "\n",
        "  #make predictions\n",
        "\n",
        "  pred_df = []\n",
        "  for i in range(N_in_ensemble):\n",
        "    tabular_model.fit(train = df_train.iloc[indexes[i], :], validation=df_valid.iloc[indexes_valid[i], :])\n",
        "    pred_df.append(tabular_model.predict(df_test, ret_logits=False))\n",
        "  \n",
        "  return pred_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FT-Transformer"
      ],
      "metadata": {
        "id": "iLuaC9eE5uTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_data = np.arange(-5.1, 5, 0.01)\n",
        "# y_data = np.add(x_data ** 2, np.multiply(0.3 * (x_data)**2, np.random.standard_normal(x_data.shape)))\n",
        "\n",
        "# plt.scatter(x_data, y_data);\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "# df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "# df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "# df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})\n",
        "\n",
        "# # parameters\n",
        "# N_in_ensemble = 2\n",
        "# batch_size = 32\n",
        "# num_col_names = ['col1']\n",
        "# cat_col_names = ['cat1']\n",
        "# epochs = 100\n",
        "# GPU = -1\n",
        "# early_stopping_patience_for_trainer_config = 5\n",
        "# auto_lr_find_for_trainer_config = False\n",
        "\n",
        "\n",
        "# # indexes\n",
        "# indexes = np.array([random.sample(range(0, int(len(df_train))), int(len(df_train)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "# indexes_valid = np.array([random.sample(range(0, int(len(df_valid))), int(len(df_valid)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "\n",
        "# steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "\n",
        "\n",
        "# # data settings\n",
        "# data_config = DataConfig(\n",
        "#   target=['target'],\n",
        "#   continuous_cols=num_col_names,\n",
        "#   categorical_cols=cat_col_names)\n",
        "\n",
        "# # define trainer configuration\n",
        "# trainer_config = TrainerConfig(\n",
        "#   auto_lr_find = auto_lr_find_for_trainer_config, # Runs the LRFinder to automatically derive a learning rate\n",
        "#   batch_size = batch_size,\n",
        "#   max_epochs = epochs,\n",
        "#   early_stopping_patience = early_stopping_patience_for_trainer_config,\n",
        "#   gpus=GPU,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "# )\n",
        "# optimizer_config_fttransformer = OptimizerConfig(lr_scheduler=\"ExponentialLR\", lr_scheduler_params={\"gamma\":0.9})\n",
        "\n",
        "# model_config_fttransformer = FTTransformerConfig(task = \"regression\",\n",
        "#                                                   learning_rate=1e-4,\n",
        "#                                                   seed = 42,\n",
        "#                                                   input_embed_dim = 16,\n",
        "#                                                   num_heads = 8,\n",
        "#                                                   num_attn_blocks = 20,\n",
        "#                                                   ff_dropout = 0.25,\n",
        "#                                                   out_ff_layers = \"1024-1024-512\",\n",
        "#                                                   out_ff_activation = \"LeakyReLU\",\n",
        "#                                                   attn_dropout=0.25,\n",
        "#                                                   embedding_dropout = 0.25,\n",
        "#                                                   #out_ff_activation = \"ReLU\",\n",
        "#                                                   out_ff_initialization=\"kaiming\",\n",
        "#                                                   batch_norm_continuous_input=False,\n",
        "#                                                   output_dim = 2                                               \n",
        "#                                               )\n",
        "\n",
        "\n",
        "# # construct model\n",
        "# tabular_model = TabularModel(\n",
        "#     data_config = data_config,\n",
        "#     model_config = model_config_fttransformer,\n",
        "#     optimizer_config = optimizer_config_fttransformer,\n",
        "#     trainer_config = trainer_config)\n",
        "\n",
        "# pred_df_x2 = []\n",
        "# for i in range(N_in_ensemble):\n",
        "#   tabular_model.fit(train = df_train.iloc[indexes[i], :], validation=df_valid.iloc[indexes_valid[i], :])\n",
        "#   pred_df_x2.append(tabular_model.predict(df_test, ret_logits=False))"
      ],
      "metadata": {
        "id": "mR1k8TlTj4N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.scatter(pred_df_x2[0]['col1'], pred_df_x2[0]['target_prediction'])"
      ],
      "metadata": {
        "id": "d7w9jxhjj4Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.scatter(df_test['col1'], df_test['target'])"
      ],
      "metadata": {
        "id": "qFwjIjy4j4Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_gap = pd.DataFrame({'col1':np.arange(5,7, 0.02), 'cat1':np.array([0]*100), 'target':np.array([5]*100)})\n"
      ],
      "metadata": {
        "id": "qj5w-9C056a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT-Transformer dropout insert\n",
        "set_total_seed(seed = 42)\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=3000)\n",
        "df_train_1, df_valid_1, df_test_1 = make_additional_data(num_samples=3000)\n",
        "\n",
        "df_train_new = pd.concat([df_train, df_train_1], axis = 0)\n",
        "df_valid_new = pd.concat([df_valid, df_valid_1], axis = 0)\n",
        "df_test_new = pd.concat([df_test, df_test_1, df_test_gap], axis = 0)\n",
        "\n",
        "\n",
        "optimizer_config_fttransformer = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "model_config_fttransformer = FTTransformerConfig(task = \"regression\",\n",
        "                                                  learning_rate=1e-4,\n",
        "                                                  seed = 42,\n",
        "                                                  input_embed_dim = 16,\n",
        "                                                  num_heads = 8,\n",
        "                                                  num_attn_blocks = 20,\n",
        "                                                  ff_dropout = 0.25,\n",
        "                                                  out_ff_layers = \"1024-1024-512\",\n",
        "                                                  out_ff_activation = \"LeakyReLU\",\n",
        "                                                  attn_dropout=0.25,\n",
        "                                                  embedding_dropout = 0.25,\n",
        "                                                  #out_ff_activation = \"ReLU\",\n",
        "                                                  out_ff_initialization=\"kaiming\",\n",
        "                                                  batch_norm_continuous_input=False\n",
        "                                              )\n",
        "\n",
        "pred_fttransformer_1 = setting_model_and_getting_predictions(model_config = model_config_fttransformer,\n",
        "                                                            optimizer_config = optimizer_config_fttransformer)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_fttransformer_1[i]['col1'], pred_fttransformer_1[i]['target_prediction'], label = 'FT-Transformer prediction (with LeakyReLU activation. {})'.format(i));\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font);\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font);\n",
        "plt.title(\"Heteroscedastic and continuous dataset. FT-Transformer\", fontdict = font_title);\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "85APU_NXotdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(np.array(col_1[0]), np.array(pred_1).mean(axis = 0))"
      ],
      "metadata": {
        "id": "Y2N32i442Fml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZUJr0_TV2bZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pred_1 = []\n",
        "col_1 = []\n",
        "for i in range(len(pred_fttransformer_1)):\n",
        "  pred_1.append(pred_fttransformer_1[i]['target_prediction'])\n",
        "  col_1.append(pred_fttransformer_1[i]['col1'])\n",
        "\n",
        "mean = np.array(pred_1).mean(axis = 0)\n",
        "upper = np.array(pred_1).mean(axis = 0) + 2 * np.array(pred_1).std(axis = 0)\n",
        "lower = np.array(pred_1).mean(axis = 0) - 2 * np.array(pred_1).std(axis = 0)\n",
        "\n",
        "plt.figure(figsize = (14,7))\n",
        "plt.scatter(col_1[0], upper, color = \"green\", alpha=.8, label = 'Mean + 2Std')\n",
        "plt.scatter(col_1[0], lower, color = \"green\", alpha=.8, label = 'Mean - 2Std')\n",
        "plt.scatter(col_1[0], mean, color = \"blue\", alpha = 1, label='Mean')\n",
        "plt.scatter(col_1[0],  df_test['target'], label = 'True', color = 'black', alpha = 0.6)\n",
        "\n",
        "#plt.fill_between(np.arange(0, 5, 5/188), (mean-(mean - lower)), (mean+(upper-mean)), color='green', alpha=.3, label = \"Cl=95%\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "vyMsW-rh0Y6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT-Transformer dropout insert\n",
        "\n",
        "set_total_seed(seed = 42)\n",
        "\n",
        "plt.figure(figsize=(13,10))\n",
        "df_test_gap = pd.DataFrame({'col1':np.arange(5,9, 0.02), 'cat1':np.array([0]*200), 'target':np.array([5]*200)})\n",
        "df_train, df_valid, df_test = make_data(num_samples=2000)\n",
        "df_train_1, df_valid_1, df_test_1 = make_additional_data(num_samples=2000, shift = 9)\n",
        "\n",
        "df_train = pd.concat([df_train, df_train_1], axis = 0)\n",
        "df_valid = pd.concat([df_valid, df_valid_1], axis = 0)\n",
        "df_test = pd.concat([df_test, df_test_1, df_test_gap], axis = 0)\n",
        "\n",
        "\n",
        "optimizer_config_fttransformer = OptimizerConfig(lr_scheduler=\"ExponentialLR\", lr_scheduler_params={\"gamma\":0.98})\n",
        "model_config_fttransformer = FTTransformerConfig(task = \"regression\",\n",
        "                                                  learning_rate=1e-4,\n",
        "                                                  seed = 42,\n",
        "                                                  input_embed_dim = 16,\n",
        "                                                  num_heads = 8,\n",
        "                                                  num_attn_blocks = 20,\n",
        "                                                  ff_dropout = 0.0,\n",
        "                                                  out_ff_layers = \"1024-1024-512\",\n",
        "                                                  out_ff_activation = \"LeakyReLU\",\n",
        "                                                  attn_dropout=0.0,\n",
        "                                                  embedding_dropout = 0.0,\n",
        "                                                  #out_ff_activation = \"ReLU\",\n",
        "                                                  out_ff_initialization=\"kaiming\",\n",
        "                                                  batch_norm_continuous_input=False\n",
        "                                              )\n",
        "\n",
        "pred_fttransformer = setting_model_and_getting_predictions(model_config = model_config_fttransformer,\n",
        "                                                            optimizer_config = optimizer_config_fttransformer)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_fttransformer[i]['col1'], pred_fttransformer[i]['target_prediction'], label = 'FT-Transformer prediction (with LeakyReLU activation. {})'.format(i));\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font);\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font);\n",
        "plt.title(\"Heteroscedastic and continuous dataset. FT-Transformer\", fontdict = font_title);\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "Aoxk0B0V9t0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_2 = []\n",
        "col_1 = []\n",
        "for i in range(len(pred_fttransformer)):\n",
        "  pred_2.append(pred_fttransformer[i]['target_prediction'])\n",
        "col_1.append(pred_fttransformer[i]['col1'])\n",
        "\n",
        "mean = np.array(pred_2).mean(axis = 0)\n",
        "upper = np.array(pred_2).mean(axis = 0) + 2 * np.array(pred_2).std(axis = 0)\n",
        "lower = np.array(pred_2).mean(axis = 0) - 2 * np.array(pred_2).std(axis = 0)\n",
        "\n",
        "plt.figure(figsize = (14,7))\n",
        "plt.scatter(col_1[0],  df_test['target'], label = \"True\", color = 'black', alpha = 0.6)\n",
        "plt.scatter(col_1[0], upper, color = \"green\", alpha=.8, label = 'Mean + 2Std')\n",
        "plt.scatter(col_1[0], lower, color = \"green\", alpha=.8, label = 'Mean - 2Std')\n",
        "plt.scatter(col_1[0], mean, color = \"blue\", alpha = 1, label='Mean')\n",
        "\n",
        "\n",
        "#plt.fill_between(np.arange(0, 5, 5/188), (mean-(mean - lower)), (mean+(upper-mean)), color='green', alpha=.3, label = \"Cl=95%\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "SWBl54DU4JdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = pd.concat([make_data(num_samples=3000)[2], make_additional_data(num_samples=3000)[2]], axis = 0)\n",
        "\n",
        "pred_2 = []\n",
        "col_1 = []\n",
        "for i in range(len(pred_fttransformer)):\n",
        "  pred_2.append(pred_fttransformer[i]['target_prediction'])\n",
        "col_1.append(pred_fttransformer[i]['col1'])\n",
        "\n",
        "mean = np.array(pred_2).mean(axis = 0)\n",
        "upper = np.array(pred_2).mean(axis = 0) + 2 * np.array(pred_2).std(axis = 0)\n",
        "lower = np.array(pred_2).mean(axis = 0) - 2 * np.array(pred_2).std(axis = 0)\n",
        "\n",
        "plt.figure(figsize = (14,7))\n",
        "plt.scatter(dt['col1'],  dt['target'], label = \"True\", color = 'black', alpha = 0.6)\n",
        "plt.scatter(col_1[0], upper, color = \"green\", alpha=.8, label = 'Mean + 2Std')\n",
        "plt.scatter(col_1[0], lower, color = \"green\", alpha=.8, label = 'Mean - 2Std')\n",
        "plt.scatter(col_1[0], mean, color = \"blue\", alpha = 1, label='Mean')\n",
        "\n",
        "\n",
        "#plt.fill_between(np.arange(0, 5, 5/188), (mean-(mean - lower)), (mean+(upper-mean)), color='green', alpha=.3, label = \"Cl=95%\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "5AYRSG9OCzSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking epistemic unsertainty\n",
        "\n",
        "df_test_gap = pd.DataFrame({'col1':np.arange(5,7, 0.02), 'cat1':np.array([0]*100), 'target':np.array([5]*100)})\n",
        "df_test_gap_1 = pd.DataFrame({'col1':np.arange(-6,0, 0.02), 'cat1':np.array([0]*300), 'target':np.array([5]*300)})\n",
        "df_test_gap_2 = pd.DataFrame({'col1':np.arange(14,20, 0.02), 'cat1':np.array([0]*300), 'target':np.array([5]*300)})\n",
        "df_test_gap = pd.concat([df_test_gap, df_test_gap_1, df_test_gap_2], axis = 0)\n",
        "\n",
        "set_total_seed(seed = 42)\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=3000)\n",
        "df_train_1, df_valid_1, df_test_1 = make_additional_data(num_samples=3000)\n",
        "\n",
        "df_train = pd.concat([df_train, df_train_1], axis = 0)\n",
        "df_valid = pd.concat([df_valid, df_valid_1], axis = 0)\n",
        "df_test = pd.concat([df_test, df_test_1, df_test_gap], axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optimizer_config_fttransformer = OptimizerConfig(lr_scheduler=\"ExponentialLR\", lr_scheduler_params={\"gamma\":0.98})\n",
        "model_config_fttransformer = FTTransformerConfig(task = \"regression\",\n",
        "                                                  learning_rate=1e-4,\n",
        "                                                  seed = 42,\n",
        "                                                  input_embed_dim = 16,\n",
        "                                                  num_heads = 8,\n",
        "                                                  num_attn_blocks = 20,\n",
        "                                                  ff_dropout = 0.0,\n",
        "                                                  out_ff_layers = \"1024-1024-512\",\n",
        "                                                  out_ff_activation = \"LeakyReLU\",\n",
        "                                                  attn_dropout=0.0,\n",
        "                                                  embedding_dropout = 0.0,\n",
        "                                                  #out_ff_activation = \"ReLU\",\n",
        "                                                  out_ff_initialization=\"kaiming\",\n",
        "                                                  batch_norm_continuous_input=False\n",
        "                                              )\n",
        "\n",
        "pred_fttransformer_epistemic = setting_model_and_getting_predictions(model_config = model_config_fttransformer,\n",
        "                                                            optimizer_config = optimizer_config_fttransformer)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_fttransformer_epistemic[i]['col1'], pred_fttransformer_epistemic[i]['target_prediction'],\n",
        "              label = 'FT-Transformer prediction (with LeakyReLU activation. {})'.format(i));\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font);\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font);\n",
        "plt.title(\"Heteroscedastic and continuous dataset. FT-Transformer\", fontdict = font_title);\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "oVAgOeYB8h_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_3 = []\n",
        "col_1 = []\n",
        "df_test_new_1 = pd.concat([make_data(num_samples=3000)[2], make_additional_data(num_samples=3000)[2]], axis = 0)\n",
        "for i in range(len(pred_fttransformer_epistemic)):\n",
        "  pred_3.append(pred_fttransformer_epistemic[i]['target_prediction'])\n",
        "col_1.append(pred_fttransformer_epistemic[i]['col1'])\n",
        "\n",
        "mean = np.array(pred_3).mean(axis = 0)\n",
        "upper = np.array(pred_3).mean(axis = 0) + 2 * np.array(pred_3).std(axis = 0)\n",
        "lower = np.array(pred_3).mean(axis = 0) - 2 * np.array(pred_3).std(axis = 0)\n",
        "\n",
        "plt.figure(figsize = (18,15))\n",
        "plt.scatter(df_test_new_1['col1'],  df_test_new_1['target'], label = \"True\", color = 'black', alpha = 0.6)\n",
        "plt.scatter(col_1[0], upper, color = \"green\", alpha=.8, label = 'Mean + 2Std')\n",
        "plt.scatter(col_1[0], lower, color = \"green\", alpha=.8, label = 'Mean - 2Std')\n",
        "plt.scatter(col_1[0], mean, color = \"blue\", alpha = 1, label='Mean')\n",
        "plt.ylim((-25, 75))\n",
        "\n",
        "\n",
        "#plt.fill_between(np.arange(0, 5, 5/188), (mean-(mean - lower)), (mean+(upper-mean)), color='green', alpha=.3, label = \"Cl=95%\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "1EvUcIr38iZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lhIhzb668ib2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT-Transformer dropout insert very high dropout\n",
        "\n",
        "set_total_seed(seed = 42)\n",
        "\n",
        "plt.figure(figsize=(13,10))\n",
        "df_test_gap = pd.DataFrame({'col1':np.arange(5,9, 0.02), 'cat1':np.array([0]*200), 'target':np.array([5]*200)})\n",
        "df_train, df_valid, df_test = make_data(num_samples=2000)\n",
        "df_train_1, df_valid_1, df_test_1 = make_additional_data(num_samples=2000, shift = 9)\n",
        "\n",
        "df_train = pd.concat([df_train, df_train_1], axis = 0)\n",
        "df_valid = pd.concat([df_valid, df_valid_1], axis = 0)\n",
        "df_test = pd.concat([df_test, df_test_1, df_test_gap], axis = 0)\n",
        "\n",
        "\n",
        "optimizer_config_fttransformer = OptimizerConfig(lr_scheduler=\"ExponentialLR\", lr_scheduler_params={\"gamma\":0.7})\n",
        "model_config_fttransformer = FTTransformerConfig(task = \"regression\",\n",
        "                                                  learning_rate=1e-4,\n",
        "                                                  seed = 42,\n",
        "                                                  input_embed_dim = 16,\n",
        "                                                  num_heads = 8,\n",
        "                                                  num_attn_blocks = 20,\n",
        "                                                  ff_dropout = 0.25,\n",
        "                                                  out_ff_layers = \"1024-1024-512\",\n",
        "                                                  out_ff_activation = \"LeakyReLU\",\n",
        "                                                  attn_dropout=0.25,\n",
        "                                                  embedding_dropout = 0.25,\n",
        "                                                  #out_ff_activation = \"ReLU\",\n",
        "                                                  out_ff_initialization=\"kaiming\",\n",
        "                                                  batch_norm_continuous_input=False\n",
        "                                              )\n",
        "\n",
        "pred_fttransformer = setting_model_and_getting_predictions(model_config = model_config_fttransformer,\n",
        "                                                            optimizer_config = optimizer_config_fttransformer)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_fttransformer[i]['col1'], pred_fttransformer[i]['target_prediction'], label = 'FT-Transformer prediction (with LeakyReLU activation. {})'.format(i));\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font);\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font);\n",
        "plt.title(\"Heteroscedastic and continuous dataset. FT-Transformer\", fontdict = font_title);\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "XjxQ5tsKBm_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT-Transformer\n",
        "set_total_seed(seed = 42)\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=10000)\n",
        "\n",
        "optimizer_config_fttransformer = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "model_config_fttransformer = FTTransformerConfig(task = \"regression\",\n",
        "                                                  learning_rate=1e-4,\n",
        "                                                  seed = 42,\n",
        "                                                  input_embed_dim = 40,\n",
        "                                                  num_heads = 10,\n",
        "                                                  num_attn_blocks = 20,\n",
        "                                                  ff_dropout = 0.2,\n",
        "                                                  out_ff_layers = \"256-128-128\",\n",
        "                                                  out_ff_activation = \"LeakyReLU\",\n",
        "                                                  attn_dropout=0.2,\n",
        "                                                  embedding_dropout = 0.2,\n",
        "                                                  #out_ff_activation = \"ReLU\",\n",
        "                                                  out_ff_initialization=\"kaiming\",\n",
        "                                                  batch_norm_continuous_input=False\n",
        "                                              )\n",
        "\n",
        "pred_fttransformer = setting_model_and_getting_predictions(model_config = model_config_fttransformer,\n",
        "                                                             optimizer_config = optimizer_config_fttransformer)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_fttransformer[i]['col1'], pred_fttransformer[i]['target_prediction'], label = 'FT-Transformer prediction (with ReLU activation. {})'.format(i))\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. FT-Transformer\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "mS37d-wwbAEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TabTransformer"
      ],
      "metadata": {
        "id": "bpdbo9S35UE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_total_seed(seed = 42)\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=10000)\n",
        "\n",
        "optimizer_config_tabtransformer = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "model_config_tabtransformer = TabTransformerConfig(task = \"regression\", loss = \"MSELoss\")\n",
        "\n",
        "pred_tab_transformer = setting_model_and_getting_predictions(model_config = model_config_tabtransformer,\n",
        "                                                             optimizer_config = optimizer_config_tabtransformer)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_tab_transformer[i]['col1'], pred_tab_transformer[i]['target_prediction'], label = 'TabTransformer prediction (with ReLU activation. {})'.format(i))\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. TabTransformer\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "FQWDpBG75TMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Auto-int\n",
        "# FT-Transformer\n",
        "set_total_seed(seed = 42)\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=10000)\n",
        "\n",
        "optimizer_config_autoint = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "model_config_autoint = AutoIntConfig(task = \"regression\" )\n",
        "\n",
        "pred_autoint = setting_model_and_getting_predictions(model_config = model_config_autoint,\n",
        "                                                           optimizer_config = optimizer_config_autoint)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_autoint[i]['col1'], pred_autoint[i]['target_prediction'], label = 'AutoInt prediction (with ReLU activation. {})'.format(i))\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. AutoInt\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jXZ6X694bBEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Node\n",
        "\n",
        "set_total_seed(seed = 42)\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=10000)\n",
        "\n",
        "optimizer_config_node = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "model_config_node = NodeConfig(task = \"regression\")\n",
        "\n",
        "pred_node = setting_model_and_getting_predictions(model_config = model_config_node,\n",
        "                                                  optimizer_config = optimizer_config_node)\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "\n",
        "#as previously defined\n",
        "N_in_ensemble = 5\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_node[i]['col1'], pred_node[i]['target_prediction'], label = 'Node prediction (with ReLU activation. {})'.format(i))\n",
        "\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. Node\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t7oIXa-KdiJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def setting_model_and_getting_predictions_dif_seeds(N_in_ensemble = 7, epochs = 10, \n",
        "#                                                     batch_size = 32,\n",
        "#                                                     num_col_names = ['col1'],\n",
        "#                                                     cat_col_names = ['cat1'], \n",
        "#                                                     auto_lr_find_for_trainer_config = False,\n",
        "#                                                     early_stopping_patience_for_trainer_config = 5,\n",
        "#                                                     GPU = -1):\n",
        "  \n",
        "#   pred_df = []\n",
        "#   for seed in np.arange(N_in_ensemble):\n",
        "#     # set seed\n",
        "#     set_total_seed(seed = seed)\n",
        "    \n",
        "#     # Data settings\n",
        "#     data_config = DataConfig(\n",
        "#       target=['target'],\n",
        "#       continuous_cols=num_col_names,\n",
        "#       categorical_cols=cat_col_names)\n",
        "    \n",
        "#     # Optimizer configuration\n",
        "#     optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "#     # define trainer configuration\n",
        "#     trainer_config = TrainerConfig(\n",
        "#       auto_lr_find = auto_lr_find_for_trainer_config, # Runs the LRFinder to automatically derive a learning rate\n",
        "#       batch_size = batch_size,\n",
        "#       max_epochs = epochs,\n",
        "#       early_stopping_patience = early_stopping_patience_for_trainer_config,\n",
        "#       gpus=GPU,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "#     )\n",
        "\n",
        "    \n",
        "\n",
        "#     # construct model\n",
        "#     tabular_model = TabularModel(\n",
        "#         data_config = data_config,\n",
        "#         model_config = model_config,\n",
        "#         optimizer_config = optimizer_config,\n",
        "#         trainer_config = trainer_config\n",
        "#     )\n",
        "\n",
        "#     #make predictions\n",
        "\n",
        "#     tabular_model.fit(train = df_train, validation=df_valid)\n",
        "#     pred_df.append(tabular_model.predict(df_test, ret_logits=False))\n",
        "\n",
        "#   return pred_df"
      ],
      "metadata": {
        "id": "NSAAP7irPrMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tkL51MUBS1Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 1 (FT-Transformer, AutoInt, Tabnet)"
      ],
      "metadata": {
        "id": "QSe4NeeaS1uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data making\n",
        "df_train, df_valid, df_test = make_data(num_samples=10000)\n",
        "###### Parameters\n",
        "N_in_ensemble = 7\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "auto_lr_find_for_trainer_config = False\n",
        "early_stopping_patience_for_trainer_config = 5\n",
        "GPU = -1\n",
        "\n",
        "######\n",
        "\n",
        "\n",
        "\n",
        "pred_df_fttransformer = []\n",
        "pred_df_autoint = []\n",
        "pred_df_tabtransformer = []\n",
        "\n",
        "\n",
        "for seed in [1,2,3,4,5,6,7]:\n",
        "    \n",
        "    # set seed\n",
        "    #set_total_seed(seed = seed)\n",
        "    \n",
        "    # Data settings\n",
        "    data_config = DataConfig(\n",
        "      target=['target'],\n",
        "      continuous_cols=num_col_names,\n",
        "      categorical_cols=cat_col_names)\n",
        "    \n",
        "    # Optimizer configuration\n",
        "    optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "    # Model config\n",
        "    model_config_ftt_ransfomer = FTTransformerConfig(task = \"regression\",\n",
        "                                                    learning_rate=1e-4,\n",
        "                                                    seed = seed,\n",
        "                                                    input_embed_dim = 40,                                          \n",
        "                                                    num_heads = 10,\n",
        "                                                    num_attn_blocks = 20,\n",
        "                                                    ff_dropout = 0.2,\n",
        "                                                    out_ff_layers = \"256-128-128\",\n",
        "                                                    out_ff_activation = \"LeakyReLU\",\n",
        "                                                    attn_dropout=0.2,\n",
        "                                                    embedding_dropout = 0.2,\n",
        "                                                    #out_ff_activation = \"ReLU\",\n",
        "                                                    out_ff_initialization=\"kaiming\",\n",
        "                                                    batch_norm_continuous_input=False)\n",
        "    \n",
        "    model_config_autoint = AutoIntConfig(task = \"regression\", seed = seed)\n",
        "    model_config_tabtransformer = TabTransformerConfig(task = \"regression\", loss = \"MSELoss\", seed = seed)\n",
        "\n",
        "\n",
        "    # define trainer configuration\n",
        "    trainer_config = TrainerConfig(\n",
        "      auto_lr_find = auto_lr_find_for_trainer_config, # Runs the LRFinder to automatically derive a learning rate\n",
        "      batch_size = batch_size,\n",
        "      max_epochs = epochs,\n",
        "      early_stopping_patience = early_stopping_patience_for_trainer_config,\n",
        "      gpus=GPU,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "    # construct model\n",
        "    tabular_model_fttransformer = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_ftt_ransfomer,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    tabular_model_autoint = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_autoint,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    tabular_model_tabtransformer = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_tabtransformer,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    \n",
        "    # fit\n",
        "    tabular_model_fttransformer.fit(train = df_train, validation=df_valid)\n",
        "    tabular_model_autoint.fit(train = df_train, validation=df_valid)\n",
        "    tabular_model_tabtransformer.fit(train = df_train, validation=df_valid)\n",
        "\n",
        "    # make predictions\n",
        "    pred_df_fttransformer.append(tabular_model_fttransformer.predict(df_test, ret_logits=False))\n",
        "    pred_df_autoint.append(tabular_model_autoint.predict(df_test, ret_logits=False))\n",
        "    pred_df_tabtransformer.append(tabular_model_tabtransformer.predict(df_test, ret_logits=False))"
      ],
      "metadata": {
        "id": "M4IJYYo_Prw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(22, 12))\n",
        "\n",
        "# First raw\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df_fttransformer[i]['col1'], pred_df_fttransformer[i]['target_prediction'], label = 'FT-Transformer prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(underexposed)\n",
        "plt.title('FT-Transformer')\n",
        "\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df_autoint[i]['col1'], pred_df_autoint[i]['target_prediction'], label = 'AutoInt prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(properly_exposed)\n",
        "plt.title('AutoInt')\n",
        "\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df_tabtransformer[i]['col1'], pred_df_tabtransformer[i]['target_prediction'], label = 'TabTransformer prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(properly_exposed)\n",
        "plt.title('TabTransformer')"
      ],
      "metadata": {
        "id": "SeJ_Izr_PrzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dqke3cK7DRq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "91pOGtx3DSF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P2tJ0yGpDSIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rb11ieJsDSLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def c(y_pred, y_true):\n",
        "    print(y_pred.dtype)\n",
        "    N = y_true.shape[0]\n",
        "    se = torch.pow((y_true[:,0]-y_pred[:,0]),2)\n",
        "    inv_std = torch.exp(-y_pred[:,1])\n",
        "    mse = torch.mean(inv_std*se)\n",
        "    reg = torch.mean(y_pred[:,1])\n",
        "    return 0.5*(mse + reg)"
      ],
      "metadata": {
        "id": "wnxXTOzWu5mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplier data\n",
        "\n",
        "# aleatoric loss function\n",
        "def aleatoric_loss(y_pred, y_true):\n",
        "    print(y_pred.dtype)\n",
        "    N = y_true.shape[0]\n",
        "    se = torch.pow((y_true[:,0]-y_pred[:,0]),2)\n",
        "    inv_std = torch.exp(-y_pred[:,1])\n",
        "    mse = torch.mean(inv_std*se)\n",
        "    reg = torch.mean(y_pred[:,1])\n",
        "    return 0.5*(mse + reg)\n",
        "\n",
        "\n",
        "# data making\n",
        "df_train, df_valid, df_test = make_data(num_samples=10000)\n",
        "###### Parameters\n",
        "N_in_ensemble = 3\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "auto_lr_find_for_trainer_config = False\n",
        "early_stopping_patience_for_trainer_config = 5\n",
        "GPU = -1\n",
        "\n",
        "######\n",
        "\n",
        "\n",
        "\n",
        "pred_df_fttransformer = []\n",
        "pred_df_autoint = []\n",
        "pred_df_tabtransformer = []\n",
        "\n",
        "\n",
        "for seed in [1,2,3]:\n",
        "    \n",
        "    # set seed\n",
        "    #set_total_seed(seed = seed)\n",
        "    \n",
        "    # Data settings\n",
        "    data_config = DataConfig(\n",
        "      target=['target'],\n",
        "      continuous_cols=num_col_names,\n",
        "      categorical_cols=cat_col_names)\n",
        "    \n",
        "    # Optimizer configuration\n",
        "    optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "    # Model config\n",
        "    model_config_ftt_ransfomer = FTTransformerConfig(task = \"regression\",\n",
        "                                                    learning_rate=1e-4,\n",
        "                                                    seed = seed,\n",
        "                                                    input_embed_dim = 40,                                          \n",
        "                                                    num_heads = 10,\n",
        "                                                    num_attn_blocks = 20,\n",
        "                                                    ff_dropout = 0.2,\n",
        "                                                    out_ff_layers = \"256-128-128\",\n",
        "                                                    out_ff_activation = \"LeakyReLU\",\n",
        "                                                    attn_dropout=0.2,\n",
        "                                                    embedding_dropout = 0.2,\n",
        "                                                    #out_ff_activation = \"ReLU\",\n",
        "                                                    out_ff_initialization=\"kaiming\",\n",
        "                                                    batch_norm_continuous_input=False,\n",
        "                                                    loss = aleatoric_loss)\n",
        "    \n",
        "    model_config_autoint = AutoIntConfig(task = \"regression\", loss = aleatoric_loss, seed = seed)\n",
        "    model_config_tabtransformer = TabTransformerConfig(task = \"regression\", loss = aleatoric_loss, seed = seed)\n",
        "\n",
        "\n",
        "    # define trainer configuration\n",
        "    trainer_config = TrainerConfig(\n",
        "      auto_lr_find = auto_lr_find_for_trainer_config, # Runs the LRFinder to automatically derive a learning rate\n",
        "      batch_size = batch_size,\n",
        "      max_epochs = epochs,\n",
        "      early_stopping_patience = early_stopping_patience_for_trainer_config,\n",
        "      gpus=GPU,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "    # construct model\n",
        "    tabular_model_fttransformer = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_ftt_ransfomer,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    tabular_model_autoint = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_autoint,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    tabular_model_tabtransformer = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_tabtransformer,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    \n",
        "    # fit\n",
        "    tabular_model_fttransformer.fit(train = df_train, validation=df_valid)\n",
        "    tabular_model_autoint.fit(train = df_train, validation=df_valid)\n",
        "    tabular_model_tabtransformer.fit(train = df_train, validation=df_valid)\n",
        "\n",
        "    # make predictions\n",
        "    pred_df_fttransformer.append(tabular_model_fttransformer.predict(df_test, ret_logits=False))\n",
        "    pred_df_autoint.append(tabular_model_autoint.predict(df_test, ret_logits=False))\n",
        "    pred_df_tabtransformer.append(tabular_model_tabtransformer.predict(df_test, ret_logits=False))"
      ],
      "metadata": {
        "id": "ye0CeoJRsO9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OydpVxthsPUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Wy1YagKsPW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tabnet and Node adding"
      ],
      "metadata": {
        "id": "q6T0-6ebVJSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_valid, df_test = make_data(num_samples=5000)\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "auto_lr_find_for_trainer_config = False\n",
        "early_stopping_patience_for_trainer_config = 5\n",
        "GPU = -1\n",
        "\n",
        "pred_df_tabnet = []\n",
        "pred_df_node = []\n",
        "\n",
        "\n",
        "for seed in [1,2,3]:\n",
        "    \n",
        "    \n",
        "    # Data settings\n",
        "    data_config = DataConfig(\n",
        "      target=['target'],\n",
        "      continuous_cols=num_col_names,\n",
        "      categorical_cols=cat_col_names)\n",
        "    \n",
        "    # Optimizer configuration\n",
        "    optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "    # Model config\n",
        "    model_config_node = NodeConfig(num_layers = 5, num_trees = 100, input_dropout = 0.2,\n",
        "                                   depth = 10, task = \"regression\", seed = seed)\n",
        "    model_config_tabnet = TabNetModelConfig(learning_rate = 0.001, n_d = 1, n_steps = 20,\n",
        "                                            task = \"regression\", seed = seed)\n",
        "\n",
        "\n",
        "    # define trainer configuration\n",
        "    trainer_config = TrainerConfig(\n",
        "      auto_lr_find = auto_lr_find_for_trainer_config, # Runs the LRFinder to automatically derive a learning rate\n",
        "      batch_size = batch_size,\n",
        "      max_epochs = epochs,\n",
        "      early_stopping_patience = early_stopping_patience_for_trainer_config,\n",
        "      gpus=GPU,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "    # construct model\n",
        "    tabular_model_node = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_node,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "    tabular_model_tabnet = TabularModel(\n",
        "        data_config = data_config,\n",
        "        model_config = model_config_tabnet,\n",
        "        optimizer_config = optimizer_config,\n",
        "        trainer_config = trainer_config\n",
        "    )\n",
        "\n",
        "\n",
        "    \n",
        "    # fit\n",
        "    tabular_model_node.fit(train = df_train, validation=df_valid)\n",
        "    tabular_model_tabnet.fit(train = df_train, validation=df_valid)\n",
        "    \n",
        "\n",
        "    # make predictions\n",
        "    pred_df_tabnet.append(tabular_model_tabnet.predict(df_test, ret_logits=False))\n",
        "    pred_df_node.append(tabular_model_node.predict(df_test, ret_logits=False))\n",
        "    "
      ],
      "metadata": {
        "id": "oWHSe53OVIrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "N_in_ensemble = 3\n",
        "# Set figure size\n",
        "plt.figure(figsize=(22, 12))\n",
        "\n",
        "# First raw\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df_node[i]['col1'], pred_df_node[i]['target_prediction'], label = 'Node prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(underexposed)\n",
        "plt.title('Node')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df_tabnet[i]['col1'], pred_df_tabnet[i]['target_prediction'], label = 'Tabnet prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(properly_exposed)\n",
        "plt.title('Tabnet')\n"
      ],
      "metadata": {
        "id": "uNOvOEzNVIts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MmBdqq3xVIGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3nOjOu4Bf8vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "84j14g4Nf8yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HFVXg11cf81C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data definition\n",
        "plt.figure(figsize=(13,10))\n",
        "df_train, df_valid, df_test = make_data(num_samples=1000)\n",
        "\n",
        "N_in_ensemble = 7\n",
        "\n",
        "# Set configurations\n",
        "model_config_ftt_ransfomer = FTTransformerConfig(task = \"regression\",\n",
        "                                          learning_rate=1e-4,\n",
        "                                          input_embed_dim = 40,                                          \n",
        "                                          num_heads = 10,\n",
        "                                          num_attn_blocks = 20,\n",
        "                                          ff_dropout = 0.2,\n",
        "                                          out_ff_layers = \"256-128-128\",\n",
        "                                          out_ff_activation = \"LeakyReLU\",\n",
        "                                          attn_dropout=0.2,\n",
        "                                          embedding_dropout = 0.2,\n",
        "                                          #out_ff_activation = \"ReLU\",\n",
        "                                          out_ff_initialization=\"kaiming\",\n",
        "                                          batch_norm_continuous_input=False)\n",
        "\n",
        "model_config_autoint = AutoIntConfig(task = \"regression\")\n",
        "model_config_tabtransformer = TabTransformerConfig(task = \"regression\", loss = \"MSELoss\")\n",
        "\n",
        "# Predictions\n",
        "pred_ftt_ransformer = setting_model_and_getting_predictions_dif_seeds(model_config = model_config_ftt_ransfomer)\n",
        "pred_autoint = setting_model_and_getting_predictions_dif_seeds(model_config = model_config_autoint)\n",
        "pred_tab_transformer = setting_model_and_getting_predictions_dif_seeds(model_config = model_config_tabtransformer)\n",
        "\n",
        "\n",
        "\n",
        "# Plotting\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(22, 12))\n",
        "\n",
        "# First raw\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_ftt_ransformer[i]['col1'], pred_ftt_ransformer[i]['target_prediction'], label = 'FT-Transformer prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(underexposed)\n",
        "plt.title('FT-Transformer')\n",
        "\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_autoint[i]['col1'], pred_autoint[i]['target_prediction'], label = 'AutoInt prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(properly_exposed)\n",
        "plt.title('AutoInt')\n",
        "\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_tab_transformer[i]['col1'], pred_tab_transformer[i]['target_prediction'], label = 'TabTransformer prediction (with ReLU activation. {})'.format(i))\n",
        "plt.legend(fontsize = 12);\n",
        "#plt.imshow(properly_exposed)\n",
        "plt.title('TabTransformer')\n",
        "\n"
      ],
      "metadata": {
        "id": "3xlynvJynfQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2nKOYJ4wnfVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epistemic uncertainty"
      ],
      "metadata": {
        "id": "yz5tO4lFOWTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_3bK3swznfXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2ZpJpBmPnfZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P_wB1CfLnfb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4muPKu5SnfeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v4DcR9GwdiSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "# aleatoric loss function\n",
        "def aleatoric_loss(y_true, y_pred):\n",
        "    N = y_true.shape[0]\n",
        "    se = K.pow((y_true[:,0]-y_pred[:,0]),2)\n",
        "    inv_std = K.exp(-y_pred[:,1])\n",
        "    mse = K.mean(inv_std*se)\n",
        "    reg = K.mean(y_pred[:,1])\n",
        "    return 0.5*(mse + reg)"
      ],
      "metadata": {
        "id": "06xyCh5ObBGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def own_mse_loss(input, target, size_average=True):\n",
        "    L = (input - target) ** 2\n",
        "    return torch.mean(L) if size_average else torch.sum(L)"
      ],
      "metadata": {
        "id": "xAP0gv4ObBMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SJ8Pg2dCYHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hBprDdv3CYJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GP350VUvmUsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oukIIYU_mUux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jWC2J1K5mUxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X9yjaNn8mUzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Homoskedastic and continuous"
      ],
      "metadata": {
        "id": "UpGXUTlnCYNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT - Transformer (new data)\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "\n",
        "samples = 20000\n",
        "x_data = np.linspace(0,100, samples)\n",
        "y_data = np.concatenate((np.zeros(int(samples/2)), 3*np.ones(int(samples/2))))+ np.sin(0.1*np.linspace(0,100, samples)) + 0.03*np.linspace(0,100, samples)\n",
        "\n",
        "plt.scatter(x_data, y_data);\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})\n",
        "\n",
        "\n",
        "\n",
        "model_config = FTTransformerConfig(\n",
        "    task = \"regression\",\n",
        "    learning_rate=1e-4,\n",
        "    seed = 42,\n",
        "    input_embed_dim = 32,\n",
        "    num_heads = 16,\n",
        "    num_attn_blocks = 10,\n",
        "    ff_dropout = 0.2,\n",
        "    out_ff_layers = \"1024-512-256\",\n",
        "    out_ff_activation = \"LeakyReLU\",\n",
        "    #out_ff_activation = \"ReLU\",\n",
        "    out_ff_initialization=\"kaiming\",\n",
        "    batch_norm_continuous_input=False,\n",
        "    #         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_leakeyrelu = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_leakeyrelu.fit(train=df_train, validation=df_valid)\n",
        "\n",
        "pred_df_leakeyrelu = tabular_model_leakeyrelu.predict(df_test, ret_logits=False)\n",
        "pred_df_leakeyrelu.head()\n",
        "\n",
        "\n",
        "model_config = FTTransformerConfig(\n",
        "    task = \"regression\",\n",
        "    learning_rate=1e-4,\n",
        "    seed = 42,\n",
        "    input_embed_dim = 32,\n",
        "    num_heads = 16,\n",
        "    num_attn_blocks = 10,\n",
        "    ff_dropout = 0.2,\n",
        "    out_ff_layers = \"1024-512-256\",\n",
        "    #out_ff_activation = \"LeakyReLU\",\n",
        "    out_ff_activation = \"ReLU\",\n",
        "    out_ff_initialization=\"kaiming\",\n",
        "    batch_norm_continuous_input=False,\n",
        "    #         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LtGPsXr1QaZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tabular_model_relu_fft = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_relu_fft.fit(train=df_train, validation=df_valid)\n",
        "\n",
        "pred_df_fft = tabular_model_relu_fft.predict(df_test, ret_logits=False)\n",
        "pred_df_fft.head()"
      ],
      "metadata": {
        "id": "1VHWl_80OJZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "i_F8OlpGKTe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TabNet"
      ],
      "metadata": {
        "id": "99ugYYZr2QPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWquNQZyoXc4"
      },
      "outputs": [],
      "source": [
        "# TabNet\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = TabNetModelConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_tabnet = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_tabnet.fit(train=df_train, validation=df_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1JesRev2pE0"
      },
      "outputs": [],
      "source": [
        "pred_df_tabnet = tabular_model_tabnet.predict(df_test, ret_logits=False)\n",
        "pred_df_tabnet.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Hohoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "SuEgvvJpRx5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S9Vp-PJsRx-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Node"
      ],
      "metadata": {
        "id": "nOhNdz-x2Wy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELS4GLsaoXhF"
      },
      "outputs": [],
      "source": [
        "# Node\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = NodeConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_node = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_node.fit(train=df_train, validation=df_valid)\n",
        "pred_df_node = tabular_model_node.predict(df_test, ret_logits=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.scatter(pred_df_node['col1'], pred_df_node['target_prediction'], label = 'Node prediction', color = 'red')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and discontinuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "AgXjsX8R_MSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R5S4QYyb0UUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TabTransformer"
      ],
      "metadata": {
        "id": "EGukWF9I4M65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B10Y-fMGoXpL"
      },
      "outputs": [],
      "source": [
        "# TabTransformer\n",
        "\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = TabTransformerConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_tabtransformer = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_tabtransformer.fit(train=df_train, validation=df_valid)\n",
        "pred_df_tabtransformer = tabular_model_tabtransformer.predict(df_test, ret_logits=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.scatter(pred_df_node['col1'], pred_df_node['target_prediction'], label = 'Node prediction', color = 'red')\n",
        "plt.scatter(pred_df_tabtransformer['col1'], pred_df_tabtransformer['target_prediction'], label = 'TabTransformer prediction', color = 'yellow')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "zs2Ed-M7_fRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoInt"
      ],
      "metadata": {
        "id": "Tt_CMZqG4Slk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NehzRQkoXvt"
      },
      "outputs": [],
      "source": [
        "# AutoInt\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = AutoIntConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_autoint = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_autoint.fit(train=df_train, validation=df_valid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Vxp2Mc3-Un"
      },
      "outputs": [],
      "source": [
        "pred_df_autoint = tabular_model_autoint.predict(df_test, ret_logits=False)\n",
        "pred_df_autoint.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZigDn_f3-bb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.scatter(pred_df_node['col1'], pred_df_node['target_prediction'], label = 'Node prediction', color = 'red')\n",
        "plt.scatter(pred_df_tabtransformer['col1'], pred_df_tabtransformer['target_prediction'], label = 'TabTransformer prediction', color = 'yellow')\n",
        "plt.scatter(pred_df_autoint['col1'], pred_df_autoint['target_prediction'], label = 'AutoInt prediction', color = 'pink')\n",
        "\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-gaG2vQ3-d2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amR7Rg5fYq84"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oNVHe_gYrBl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATxm5bn3YrDe"
      },
      "outputs": [],
      "source": [
        "def uncertainity_estimate(x, model, num_samples, l2):\n",
        "    outputs = np.hstack([model.predict(x, ret_logits=False).detach().numpy() for i in range(num_samples)]) # n inference, output.shape = [20, N]\n",
        "    y_mean = outputs.mean(axis=1)\n",
        "    y_variance = outputs.var(axis=1)\n",
        "    tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)\n",
        "    y_variance += (1. / tau)\n",
        "    y_std = np.sqrt(y_variance)\n",
        "    return y_mean, y_std\n",
        "\n",
        "\n",
        "# Normalise data:\n",
        "\n",
        "x_mean, x_std = df_train['col1'].mean(), df_train['col1'].std()\n",
        "y_mean, y_std = df_train['target'].mean(), df_train['target'].std()\n",
        "x_obs = (df_train['col1'] - x_mean) / x_std\n",
        "y_obs = (df_train['target'] - y_mean) / y_std\n",
        "x_test = (df_valid['col1'] - x_mean) / x_std\n",
        "y_test = (df_valid['target'] - y_mean) / y_std\n",
        "\n",
        "iters_uncertainty = 200\n",
        "\n",
        "lengthscale = 0.01\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "n_std = 2 # number of standard deviations to plot\n",
        "y_mean, y_std = uncertainity_estimate(x = torch.Tensor(x_test).view(-1,1).to(device),\n",
        "                                      model = tabular_model, iters_uncertainty, lengthscale)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"observed\")\n",
        "plt.plot(x_test, y_mean, ls=\"-\", color=\"b\", label=\"mean\")\n",
        "plt.plot(x_test, y_test, ls='--', color='r', label='true')\n",
        "for i in range(n_std):\n",
        "    plt.fill_between( x_test,\n",
        "        y_mean - y_std * ((i+1.)),\n",
        "        y_mean + y_std * ((i+1.)),\n",
        "        color=\"b\",\n",
        "        alpha=0.1)\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5uhqywhYrFg"
      },
      "outputs": [],
      "source": [
        "def uncertainity_estimate(x, model, num_samples, l2):\n",
        "    outputs = np.hstack([model(x).cpu().detach().numpy() for i in range(num_samples)]) # n inference, output.shape = [20, N]\n",
        "    y_mean = outputs.mean(axis=1)\n",
        "    y_variance = outputs.var(axis=1)\n",
        "    tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)\n",
        "    y_variance += (1. / tau)\n",
        "    y_std = np.sqrt(y_variance)\n",
        "    return y_mean, y_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BCuNFhvVN4Z6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "12_04_embedding_STAT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}