{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies and libraries"
      ],
      "metadata": {
        "id": "9NqfvaUqCmel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2RX38Hg7sYf"
      },
      "outputs": [],
      "source": [
        "! pip install pytorch_tabular\n",
        "! git clone https://github.com/manujosephv/pytorch_tabular\n",
        "%cd pytorch_tabular\n",
        "\n",
        "!python setup.py install\n",
        "!pip install setuptools==59.5.0\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qmuotr7_UO0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if not IN_COLAB:\n",
        "    os.chdir(\"..\")\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pytorch_tabular import TabularModel\n",
        "from pytorch_tabular.models import CategoryEmbeddingModelConfig, FTTransformerConfig, FTTransformerModel, TabNetModelConfig, TabNetModel, AutoIntConfig, AutoIntConfig, TabTransformerConfig, TabTransformerModel\n",
        "from pytorch_tabular.models import AutoIntModel, AutoIntConfig, NodeConfig, NODEModel\n",
        "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig, ModelConfig\n",
        "from pytorch_tabular.models import BaseModel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from omegaconf import DictConfig\n",
        "from typing import Dict\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# for plots\n",
        "font = {'family': 'serif',\n",
        "        'color':  'darkred',\n",
        "        'weight': 'normal',\n",
        "        'size': 16\n",
        "        }\n",
        "\n",
        "font_title = {'family': 'serif',\n",
        "        'color':  'darkred',\n",
        "        'weight': 'normal',\n",
        "        'size': 20\n",
        "        }\n",
        "\n",
        "# Set random seed\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FT-Transformer architecture"
      ],
      "metadata": {
        "id": "OlVcukXkKWPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# samples = 20000\n",
        "# x_data = np.linspace(0,100, samples)\n",
        "# y_data = np.concatenate((np.zeros(int(samples/2)), 3*np.ones(int(samples/2))))+ np.sin(0.1*np.linspace(0,100, samples)) + 0.03*np.linspace(0,100, samples)\n",
        "\n",
        "# plt.scatter(x_data, y_data);\n",
        "\n",
        "samples = 20000\n",
        "x_data = np.random.sample(samples)[:, np.newaxis].astype(np.float32)*5\n",
        "y_data = np.add(10*x_data+6*x_data**2-1.5*x_data**3, np.multiply((x_data)**2, np.sin(3*x_data)*np.random.standard_normal(x_data.shape)))\n",
        "\n",
        "plt.scatter(x_data, y_data);\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})"
      ],
      "metadata": {
        "id": "8_xXNX08KTFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_in_ensemble = 10\n",
        "indexes = np.array([random.sample(range(0, int(len(df_train))), int(len(df_train)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "indexes_valid = np.array([random.sample(range(0, int(len(df_valid))), int(len(df_valid)/N_in_ensemble)) for i in range(N_in_ensemble)])\n"
      ],
      "metadata": {
        "id": "SvApXlg6uhxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT - Transformer (new data)\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "\n",
        "\n",
        "# epochs = 20\n",
        "# batch_size = 1000\n",
        "# steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "# data_config = DataConfig(\n",
        "#     target=['target'],\n",
        "#     continuous_cols=num_col_names,\n",
        "#     categorical_cols=cat_col_names,\n",
        "# #         continuous_feature_transform=\"quantile_uniform\"\n",
        "# )\n",
        "# trainer_config = TrainerConfig(\n",
        "#     auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n",
        "#     batch_size=batch_size,\n",
        "#     max_epochs=epochs,\n",
        "#     early_stopping_patience = 10,\n",
        "#     gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        "# )\n",
        "# # optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":10})\n",
        "\n",
        "\n",
        "# model_config = FTTransformerConfig(\n",
        "#     task = \"regression\",\n",
        "#     learning_rate=1e-4,\n",
        "#     seed = 42,\n",
        "#     input_embed_dim = 32,\n",
        "#     num_heads = 16,\n",
        "#     num_attn_blocks = 10,\n",
        "#     ff_dropout = 0.2,\n",
        "#     out_ff_layers = \"1024-512-256\",\n",
        "#     #out_ff_activation = \"LeakyReLU\",\n",
        "#     out_ff_activation = \"ReLU\",\n",
        "#     out_ff_initialization=\"kaiming\",\n",
        "#     batch_norm_continuous_input=False,\n",
        "#     #         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# tabular_model = TabularModel(\n",
        "#     data_config=data_config,\n",
        "#     model_config=model_config,\n",
        "#     optimizer_config=optimizer_config,\n",
        "#     trainer_config=trainer_config\n",
        "# )\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = TabTransformerConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "pred_df = []\n",
        "for i in range(N_in_ensemble):\n",
        "  tabular_model.fit(train = df_train.iloc[indexes[i], :], validation=df_valid.iloc[indexes_valid[i], :])\n",
        "  pred_df.append(tabular_model.predict(df_test, ret_logits=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "VJQQU446KTYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df[i]['col1'], pred_df[i]['target_prediction'], label = 'TabTransformer prediction (with ReLU activation. {})'.format(i))\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. TabTransformer\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "vgKqiqm54fxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT - Transformer (new data)\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "\n",
        "\n",
        "samples = 10000\n",
        "x_data = np.random.sample(samples)[:, np.newaxis].astype(np.float32)*5\n",
        "y_data = np.add(10*x_data+6*x_data**2-1.5*x_data**3, np.multiply((x_data)**2, np.sin(3*x_data)*np.random.standard_normal(x_data.shape)))\n",
        "\n",
        "plt.scatter(x_data, y_data);\n",
        "\n",
        "N_in_ensemble = 10\n",
        "indexes = np.array([random.sample(range(0, int(len(df_train))), int(len(df_train)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "indexes_valid = np.array([random.sample(range(0, int(len(df_valid))), int(len(df_valid)/N_in_ensemble)) for i in range(N_in_ensemble)])\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 10,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":10})\n",
        "\n",
        "\n",
        "model_config = FTTransformerConfig(\n",
        "    task = \"regression\",\n",
        "    learning_rate=1e-4,\n",
        "    seed = 42,\n",
        "    input_embed_dim = 8,\n",
        "    num_heads = 4,\n",
        "    num_attn_blocks = 10,\n",
        "    ff_dropout = 0.3,\n",
        "    out_ff_layers = \"64-32-32\",\n",
        "    out_ff_activation = \"LeakyReLU\",\n",
        "    #out_ff_activation = \"ReLU\",\n",
        "    out_ff_initialization=\"kaiming\",\n",
        "    batch_norm_continuous_input=False,\n",
        "    #         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_fttransformer = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "pred_df = []\n",
        "for i in range(N_in_ensemble):\n",
        "  tabular_model_fttransformer.fit(train = df_train.iloc[indexes[i], :], validation=df_valid.iloc[indexes_valid[i], :])\n",
        "  pred_df.append(tabular_model_fttransformer.predict(df_test, ret_logits=False))\n"
      ],
      "metadata": {
        "id": "ZOz-eYBMCX9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df[i]['col1'], pred_df[i]['target_prediction'], label = 'FT-TRansformer prediction (with ReLU activation. {})'.format(i))\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. FT-Transformer\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "XWAY3sjRCX-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AutoInt\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = AutoIntConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_autoint = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pred_df_autoint = []\n",
        "for i in range(N_in_ensemble):\n",
        "  tabular_model_autoint.fit(train = df_train.iloc[indexes[i], :], validation=df_valid.iloc[indexes_valid[i], :])\n",
        "  pred_df_autoint.append(tabular_model_autoint.predict(df_test, ret_logits=False))\n"
      ],
      "metadata": {
        "id": "4Tw6EM0VCYBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(df_test['col1'], df_test['target'], label = 'True', color = 'black')\n",
        "for i in range(N_in_ensemble):\n",
        "  plt.scatter(pred_df[i]['col1'], pred_df[i]['target_prediction'], label = 'AutoInt prediction {}'.format(i))\n",
        "\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"$Y = 10X+6X^{2}-1.5X^{3} + X^{2} *  sin(3X)*\\epsilon$\", fontdict = font)\n",
        "plt.title(\"Heteroscedastic and continuous dataset. AutoInt\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "STwi89TkCYDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ne7k9KjlCYFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6SJ8Pg2dCYHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hBprDdv3CYJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Homoskedastic and continuous"
      ],
      "metadata": {
        "id": "UpGXUTlnCYNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FT - Transformer (new data)\n",
        "num_col_names = ['col1']\n",
        "cat_col_names = ['cat1']\n",
        "\n",
        "samples = 20000\n",
        "x_data = np.linspace(0,100, samples)\n",
        "y_data = np.concatenate((np.zeros(int(samples/2)), 3*np.ones(int(samples/2))))+ np.sin(0.1*np.linspace(0,100, samples)) + 0.03*np.linspace(0,100, samples)\n",
        "\n",
        "plt.scatter(x_data, y_data);\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, random_state = 42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val)\n",
        "\n",
        "df_train = pd.DataFrame({'col1':X_train.flatten(), 'cat1':0, 'target':y_train.flatten()})\n",
        "df_valid = pd.DataFrame({'col1':X_val.flatten(), 'cat1':0, 'target':y_val.flatten()})\n",
        "df_test = pd.DataFrame({'col1':X_test.flatten(), 'cat1':0, 'target':y_test.flatten()})\n",
        "\n",
        "\n",
        "\n",
        "model_config = FTTransformerConfig(\n",
        "    task = \"regression\",\n",
        "    learning_rate=1e-4,\n",
        "    seed = 42,\n",
        "    input_embed_dim = 32,\n",
        "    num_heads = 16,\n",
        "    num_attn_blocks = 10,\n",
        "    ff_dropout = 0.2,\n",
        "    out_ff_layers = \"1024-512-256\",\n",
        "    out_ff_activation = \"LeakyReLU\",\n",
        "    #out_ff_activation = \"ReLU\",\n",
        "    out_ff_initialization=\"kaiming\",\n",
        "    batch_norm_continuous_input=False,\n",
        "    #         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_leakeyrelu = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_leakeyrelu.fit(train=df_train, validation=df_valid)\n",
        "\n",
        "pred_df_leakeyrelu = tabular_model_leakeyrelu.predict(df_test, ret_logits=False)\n",
        "pred_df_leakeyrelu.head()\n",
        "\n",
        "\n",
        "model_config = FTTransformerConfig(\n",
        "    task = \"regression\",\n",
        "    learning_rate=1e-4,\n",
        "    seed = 42,\n",
        "    input_embed_dim = 32,\n",
        "    num_heads = 16,\n",
        "    num_attn_blocks = 10,\n",
        "    ff_dropout = 0.2,\n",
        "    out_ff_layers = \"1024-512-256\",\n",
        "    #out_ff_activation = \"LeakyReLU\",\n",
        "    out_ff_activation = \"ReLU\",\n",
        "    out_ff_initialization=\"kaiming\",\n",
        "    batch_norm_continuous_input=False,\n",
        "    #         target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LtGPsXr1QaZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tabular_model_relu_fft = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_relu_fft.fit(train=df_train, validation=df_valid)\n",
        "\n",
        "pred_df_fft = tabular_model_relu_fft.predict(df_test, ret_logits=False)\n",
        "pred_df_fft.head()"
      ],
      "metadata": {
        "id": "1VHWl_80OJZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "i_F8OlpGKTe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TabNet"
      ],
      "metadata": {
        "id": "99ugYYZr2QPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWquNQZyoXc4"
      },
      "outputs": [],
      "source": [
        "# TabNet\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = TabNetModelConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_tabnet = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_tabnet.fit(train=df_train, validation=df_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1JesRev2pE0"
      },
      "outputs": [],
      "source": [
        "pred_df_tabnet = tabular_model_tabnet.predict(df_test, ret_logits=False)\n",
        "pred_df_tabnet.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Hohoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "SuEgvvJpRx5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S9Vp-PJsRx-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Node"
      ],
      "metadata": {
        "id": "nOhNdz-x2Wy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELS4GLsaoXhF"
      },
      "outputs": [],
      "source": [
        "# Node\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = NodeConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_node = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_node.fit(train=df_train, validation=df_valid)\n",
        "pred_df_node = tabular_model_node.predict(df_test, ret_logits=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.scatter(pred_df_node['col1'], pred_df_node['target_prediction'], label = 'Node prediction', color = 'red')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and discontinuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "AgXjsX8R_MSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R5S4QYyb0UUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TabTransformer"
      ],
      "metadata": {
        "id": "EGukWF9I4M65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B10Y-fMGoXpL"
      },
      "outputs": [],
      "source": [
        "# TabTransformer\n",
        "\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = TabTransformerConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_tabtransformer = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_tabtransformer.fit(train=df_train, validation=df_valid)\n",
        "pred_df_tabtransformer = tabular_model_tabtransformer.predict(df_test, ret_logits=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.scatter(pred_df_node['col1'], pred_df_node['target_prediction'], label = 'Node prediction', color = 'red')\n",
        "plt.scatter(pred_df_tabtransformer['col1'], pred_df_tabtransformer['target_prediction'], label = 'TabTransformer prediction', color = 'yellow')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ],
      "metadata": {
        "id": "zs2Ed-M7_fRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoInt"
      ],
      "metadata": {
        "id": "Tt_CMZqG4Slk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NehzRQkoXvt"
      },
      "outputs": [],
      "source": [
        "# AutoInt\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "steps_per_epoch = int((len(df_train)//batch_size)*0.9)\n",
        "data_config = DataConfig(\n",
        "    target=['target'],\n",
        "    continuous_cols=num_col_names,\n",
        "    categorical_cols=cat_col_names,\n",
        "#         continuous_feature_transform=\"quantile_uniform\"\n",
        ")\n",
        "trainer_config = TrainerConfig(\n",
        "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=epochs,\n",
        "    early_stopping_patience = 5,\n",
        "    gpus=-1,  #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
        ")\n",
        "# optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n",
        "\n",
        "optimizer_config = OptimizerConfig(lr_scheduler=\"ReduceLROnPlateau\", lr_scheduler_params={\"patience\":3})\n",
        "\n",
        "\n",
        "model_config = AutoIntConfig(\n",
        "    task = \"regression\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tabular_model_autoint = TabularModel(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    trainer_config=trainer_config\n",
        ")\n",
        "\n",
        "tabular_model_autoint.fit(train=df_train, validation=df_valid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Vxp2Mc3-Un"
      },
      "outputs": [],
      "source": [
        "pred_df_autoint = tabular_model_autoint.predict(df_test, ret_logits=False)\n",
        "pred_df_autoint.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZigDn_f3-bb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target_prediction'], label = 'FT-Transform prediction (with ReLU activation)', color = 'red')\n",
        "plt.scatter(pred_df_leakeyrelu['col1'], pred_df_leakeyrelu['target_prediction'], label = 'FT-Transform prediction (with LeakeyRelu activation)', color = 'blue')\n",
        "\n",
        "plt.scatter(pred_df_tabnet['col1'], pred_df_tabnet['target_prediction'], label = 'TabNet prediction', color = 'green')\n",
        "plt.scatter(pred_df_node['col1'], pred_df_node['target_prediction'], label = 'Node prediction', color = 'red')\n",
        "plt.scatter(pred_df_tabtransformer['col1'], pred_df_tabtransformer['target_prediction'], label = 'TabTransformer prediction', color = 'yellow')\n",
        "plt.scatter(pred_df_autoint['col1'], pred_df_autoint['target_prediction'], label = 'AutoInt prediction', color = 'pink')\n",
        "\n",
        "plt.scatter(pred_df_fft['col1'], pred_df_fft['target'], label = 'True', color = 'black')\n",
        "plt.xlabel(\"X\", fontdict = font)\n",
        "plt.ylabel(\"Y = sin(0.1X) + 0.03X\", fontdict = font)\n",
        "plt.title(\"Homoskedastic and continuous dataset\", fontdict = font_title)\n",
        "plt.legend(fontsize = 14);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-gaG2vQ3-d2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amR7Rg5fYq84"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oNVHe_gYrBl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATxm5bn3YrDe"
      },
      "outputs": [],
      "source": [
        "def uncertainity_estimate(x, model, num_samples, l2):\n",
        "    outputs = np.hstack([model.predict(x, ret_logits=False).detach().numpy() for i in range(num_samples)]) # n번 inference, output.shape = [20, N]\n",
        "    y_mean = outputs.mean(axis=1)\n",
        "    y_variance = outputs.var(axis=1)\n",
        "    tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)\n",
        "    y_variance += (1. / tau)\n",
        "    y_std = np.sqrt(y_variance)\n",
        "    return y_mean, y_std\n",
        "\n",
        "\n",
        "# Normalise data:\n",
        "\n",
        "x_mean, x_std = df_train['col1'].mean(), df_train['col1'].std()\n",
        "y_mean, y_std = df_train['target'].mean(), df_train['target'].std()\n",
        "x_obs = (df_train['col1'] - x_mean) / x_std\n",
        "y_obs = (df_train['target'] - y_mean) / y_std\n",
        "x_test = (df_valid['col1'] - x_mean) / x_std\n",
        "y_test = (df_valid['target'] - y_mean) / y_std\n",
        "\n",
        "iters_uncertainty = 200\n",
        "\n",
        "lengthscale = 0.01\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "n_std = 2 # number of standard deviations to plot\n",
        "y_mean, y_std = uncertainity_estimate(x = torch.Tensor(x_test).view(-1,1).to(device),\n",
        "                                      model = tabular_model, iters_uncertainty, lengthscale)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"observed\")\n",
        "plt.plot(x_test, y_mean, ls=\"-\", color=\"b\", label=\"mean\")\n",
        "plt.plot(x_test, y_test, ls='--', color='r', label='true')\n",
        "for i in range(n_std):\n",
        "    plt.fill_between( x_test,\n",
        "        y_mean - y_std * ((i+1.)),\n",
        "        y_mean + y_std * ((i+1.)),\n",
        "        color=\"b\",\n",
        "        alpha=0.1)\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5uhqywhYrFg"
      },
      "outputs": [],
      "source": [
        "def uncertainity_estimate(x, model, num_samples, l2):\n",
        "    outputs = np.hstack([model(x).cpu().detach().numpy() for i in range(num_samples)]) # n번 inference, output.shape = [20, N]\n",
        "    y_mean = outputs.mean(axis=1)\n",
        "    y_variance = outputs.var(axis=1)\n",
        "    tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)\n",
        "    y_variance += (1. / tau)\n",
        "    y_std = np.sqrt(y_variance)\n",
        "    return y_mean, y_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BCuNFhvVN4Z6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Research_full_work_discont_homosk.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}